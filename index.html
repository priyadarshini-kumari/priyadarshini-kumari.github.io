<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Priyadarshini Kumari </title> <meta name="author" content="Priyadarshini "> <meta name="description" content="My Webpage "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.priyadarshini-k.com/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%70%72%69%79%61%64%61%72%73%68%69%6E%69.%6B%75%6D%61%72%69@%73%6F%6E%79.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=mD0burkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.linkedin.com/in/priyadarshini-kumari-501b65162" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a target="_blank" href="assets/pdf/priyadarshini_cv.pdf" title="Work"><i class="ai ai-cv ai-1x"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Priyadarshini Kumari </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="(min-width: 1000px) 291.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/prof_pic.jpg?795439879e7954437429bf560e61cf32" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>I work at Apple, focusing on the intersection of machine learning and health.</p> <p>Previously at <a href="https://ai.sony/" rel="external nofollow noopener" target="_blank">Sony AI</a>, I contributed to a range of projects, from developing data-efficient machine learning techniques for graph neural networks to creating multimodal perception models integrating text, and olfactory inputs. My work spanned applications in biomedical research, olfaction, and gastronomy.</p> <p>I received my Ph.D. from <a href="https://www.iitb.ac.in/" rel="external nofollow noopener" target="_blank">IIT Bombay</a>, advised by <a href="https://www.ee.iitb.ac.in/~sc/main/main.html" rel="external nofollow noopener" target="_blank">Prof. Subhasis Chaudhuri</a> and <a href="https://www.cse.iitb.ac.in/~sidch/" rel="external nofollow noopener" target="_blank">Prof. Siddhartha Chaudhuri</a>. My thesis was on <i>Label-Efficient Distance Metric Learning</i>. Before that, I completed my master’s also from IIT Bombay where I developed multimodal rendering techniques that combined haptic, visual, and auditory feedback to make 3D models of heritage sites accessible to the visually impaired.</p> <p>Here is my <a href="assets/pdf/priyadarshini_cv.pdf">CV</a></p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jul 25, 2024</th> <td> Our paper “Link prediction for hypothesis generation: an active curriculum learning infused temporal graph-based approach” is accepted at Artificial Intelligence Review 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 12, 2024</th> <td> Our paper “CosFairNet:A Parameter-Space based Approach for Bias Free Learning” is accepted at BMVC 2024. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 31, 2023</th> <td> Our paper “FRUNI and FTREE synthetic knowledge graphs for evaluating explainability” is accepted at NeurIPS XAIA workshop 2023. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 21, 2023</th> <td> Two papers “Perceptual metrics for odorants: learning from non-expert similarity feedback using machine learning” and “Comparing molecular representations, e-nose signals, and other featurization, for learning to smell aroma molecules” are accepted at PLOS One </td> </tr> <tr> <th scope="row" style="width: 20%">Jul 15, 2023</th> <td> Our paper “Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception” was accepted to Multimodal SIGKDD 2023 </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ds1-480.webp 480w,/assets/img/publication_preview/ds1-800.webp 800w,/assets/img/publication_preview/ds1-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ds1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ds1.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge"> <a href="">AI Review</a></abbr> </div> <div id="priyadarshini2024ds1" class="col-sm-8"> <div class="title">Link prediction for hypothesis generation: an active curriculum learning infused temporal graph-based approach</div> <div class="author"> Uchenna Akujuobi* , <em>Priyadarshini Kumari</em>, Jihun Choi , Samy Badreddine , Kana Maruyama , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sucheendra K. Palaniappan, Tarek R. Besold' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Artificial Intelligence Review</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/ds1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Over the last few years Literature-based Discovery (LBD) has regained popularity as a means to enhance the scientific research process. The resurgent interest has spurred the development of supervised and semi-supervised machine learning models aimed at making previously implicit connections between scientific concepts/entities explicit based on often extensive repositories of published literature. Understanding the temporally evolving interactions between these entities can provide valuable information for predicting the future development of entity relationships. However, existing methods often underutilize the latent information embedded in the temporal aspects of interaction data. In this context, motivated by applications in the food domain—where we aim to connect nutritional information with health-related benefits—we address the hypothesis-generation problem using a temporal graph-based approach. Given that hypothesis generation involves predicting future (i.e., still to be discovered) entity connections, the ability to capture the dynamic evolution of connections over time is pivotal for a robust model. To address this, we introduce THiGER, a novel batch contrastive temporal node-pair embedding method. THiGER excels in providing a more expressive node-pair encoding by effectively harnessing node-pair relationships. Furthermore, we present THiGER-A, an incremental training approach that incorporates an active curriculum learning strategy to mitigate label bias arising from unobserved connections. By progressively training on increasingly challenging and high-utility samples, our approach significantly enhances the performance of the embedding model. Empirical validation of our proposed method demonstrates its effectiveness on established temporal-graph benchmark datasets, as well as on real-world datasets within the food domain.</p> </div> <p>We present THiGER-A, a solution aimed at addressing the hypothesis-generation problem. This method utilizes a temporal graph-based node-pair embedding and incorporates an active-curriculum training approach to precisely capture the dynamic evolution of discoveries over time.</p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fruni_ftree-480.webp 480w,/assets/img/publication_preview/fruni_ftree-800.webp 800w,/assets/img/publication_preview/fruni_ftree-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/fruni_ftree.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fruni_ftree.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge"> <a href="">NeurIPS XAIA</a></abbr> </div> <div id="priyadarshini2023neurips" class="col-sm-8"> <div class="title">FRUNI and FTREE synthetic knowledge graphs for evaluating explainability</div> <div class="author"> Pablo Sanchez Martin , Tarek Besold , and <em>Priyadarshini Kumari</em> </div> <div class="periodical"> <em>In NeurIPS 2023 Workshop XAIA</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/fruni_and_ftree.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/SonyResearch/synthetic_knowledge_graphs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Research on knowledge graph completion (KGC)—i.e., link prediction within incomplete KGs—is witnessing significant growth in popularity. Recently, KGC using KG embedding (KGE) models, primarily based on complex architectures (e.g., transformers), have achieved remarkable performance. Still, extracting the \emphminimal and relevant information employed by KGE models to make predictions, while constituting a major part of \emphexplaining the predictions, remains a challenge. While there exists a growing literature on explainers for trained KGE models, systematically exposing and quantifying their failure cases poses even greater challenges. In this work, we introduce two synthetic datasets, FRUNI and FTREE, designed to demonstrate the (in)ability of explainer methods to spot link predictions that rely on indirectly connected links. Notably, we empower practitioners to control various aspects of the datasets, such as noise levels and dataset size, enabling them to assess the performance of explainability methods across diverse scenarios. Through our experiments, we assess the performance of four recent explainers in providing accurate explanations for predictions on the proposed datasets. We believe that these datasets are valuable resources for further validating explainability methods within the knowledge graph community.</p> </div> <p>We introduce two synthetic datasets, FRUNI and FTREE, to assess explainer methods’ ability to identify predictions relying on indirectly connected links.</p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/label_balancer_method-480.webp 480w,/assets/img/publication_preview/label_balancer_method-800.webp 800w,/assets/img/publication_preview/label_balancer_method-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/label_balancer_method.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="label_balancer_method.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge"> <a href="">Multimodal SIGKDD</a></abbr> </div> <div id="priyadarshini2023aromasense" class="col-sm-8"> <div class="title">Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception</div> <div class="author"> Daniel Shin , Gao Pei , <em>Priyadarshini Kumari</em>, and Tarek Besold </div> <div class="periodical"> <em>In International Workshop on Multimodal Learning at SIGKDD</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/Multimodal_KDD_2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/priyadarshini-sony/multimodal-olfaction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a target="_blank" href="/assets/pdf/multimodal_SIGKDD_2023_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>For humans and other animals, the sense of smell provides crucial information in many situations of everyday life. Still, the study of olfactory perception has received only limited attention outside of the biological sciences. From an AI perspective, the complexity of the interactions between olfactory receptors and volatile molecules and the scarcity of comprehensive olfactory datasets, present unique challenges in this sensory domain. Previous works have explored the relationship between molecular structure and odor descriptors using fully supervised training approaches. However, these methods are data-intensive and poorly generalize due to labeled data scarcity, particularly for rare-class samples. Our study partially tackles the challenges of data scarcity and label skewness through multimodal transfer learning. We investigate the potential of large molecular foundation models trained on extensive unlabeled molecular data to effectively model olfactory perception. Additionally, we explore the integration of different molecular representations, including molecular graphs and text-based SMILES encodings, to achieve data efficiency and generalization of the learned model, particularly on sparsely represented classes. By leveraging complementary representations, we aim to learn robust perceptual features of odorants. However, we observe that traditional methods of combining modalities do not yield substantial gains in high-dimensional skewed label spaces. To address this challenge, we introduce a novel label-balancer technique specifically designed for high-dimensional multi-label and multi-modal training. The label-balancer technique distributes learning objectives across modalities to optimize collaboratively for distinct subsets of labels. Our results suggest that multi-modal transfer features learned using the label-balancer technique are more effective and robust, surpassing the capabilities of traditional uni- or multi-modal approaches, particularly on rare-class samples.</p> </div> <p>We introduce a novel multilabel and multimodal transfer learning technique for modeling olfactory perception. Our approach aims to tackle the challenges of data scarcity and label skewness in the olfactory domain.</p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ecml2021-480.webp 480w,/assets/img/publication_preview/ecml2021-800.webp 800w,/assets/img/publication_preview/ecml2021-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ecml2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ecml2021.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge"> <a href="">ECML-PKDD</a></abbr> </div> <div id="priyadarshini2021unified" class="col-sm-8"> <div class="title">A Unified Batch Selection Policy for Active Metric Learning</div> <div class="author"> <em>Priyadarshini Kumari</em>, Siddhartha Chaudhuri , Vivek Borkar , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In ECML-PKDD</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/ecml.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a target="_blank" href="/assets/pdf/ecml2021_slide.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Active metric learning is the problem of incrementally selecting high-utility batches of training data (typically, ordered triplets) to annotate, in order to progressively improve a learned model of a metric over some input domain as rapidly as possible. Standard approaches, which independently assess the informativeness of each triplet in a batch, are susceptible to highly correlated batches with many redundant triplets and hence low overall utility. While a recent work proposes batch-decorrelation strategies for metric learning, they rely on ad hoc heuristics to estimate the correlation between two triplets at a time. We present a novel batch active metric learning method that leverages the Maximum Entropy Principle to learn the least biased estimate of triplet distribution for a given set of prior constraints. To avoid redundancy between triplets, our method collectively selects batches with maximum joint entropy, which simultaneously captures both informativeness and diversity. We take advantage of the submodularity of the joint entropy function to construct a tractable solution using an efficient greedy algorithm based on Gram-Schmidt orthogonalization that is provably (1−\frac1𝑒)-optimal. Our approach is the first batch active metric learning method to define a unified score that balances informativeness and diversity for an entire batch of triplets. Experiments with several real-world datasets demonstrate that our algorithm is robust, generalizes well to different applications and input modalities, and consistently outperforms the state-of-the-art.</p> </div> <p>We propose a batch-mode active learning method that balances informativeness and diversity of batches of triplets combinedly. </p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ijcai2020-480.webp 480w,/assets/img/publication_preview/ijcai2020-800.webp 800w,/assets/img/publication_preview/ijcai2020-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/ijcai2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ijcai2020.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge"> <a href="">IJCAI</a></abbr> </div> <div id="kumari2021batch" class="col-sm-8"> <div class="title">Batch decorrelation for active metric learning</div> <div class="author"> <em>Priyadarshini Kumari</em>, Ritesh Goru , Siddhartha Chaudhuri , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In IJCAI</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/ijcai.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/priyadarshini-kumari/BatchAML_Decorrelation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a target="_blank" href="/assets/pdf/ijcai2020_slide.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>We present an active learning strategy for training parametric models of distance metrics, given triplet-based similarity assessments: object xi is more similar to object xj than to xk. In contrast to prior work on class-based learning, where the fundamental goal is classification and any implicit or explicit metric is binary, we focus on perceptual metrics that express the degree of (dis) similarity between objects. We find that standard active learning approaches degrade when annotations are requested for batches of triplets at a time: our studies suggest that correlation among triplets is responsible. In this work, we propose a novel method to decorrelate batches of triplets, that jointly balances informativeness and diversity while decoupling the choice of heuristic for each criterion. Experiments indicate our method is general, adaptable, and outperforms the state-of-the-art.</p> </div> <p>We propose triplet-based decorrelation measures to improve the performance of batch-mode active metric learning strategies. </p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/whc2019-480.webp 480w,/assets/img/publication_preview/whc2019-800.webp 800w,/assets/img/publication_preview/whc2019-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/whc2019.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="whc2019.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <abbr class="badge"> <a href="">IEEE WHC</a></abbr> </div> <div id="priyadarshini2019perceptnet" class="col-sm-8"> <div class="title">PerceptNet: Learning perceptual similarity of haptic textures in presence of unorderable triplets</div> <div class="author"> <em>Priyadarshini Kumari</em>, Siddhartha Chaudhuri , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In IEEE World Haptics Conference (WHC)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/whc2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/priyadarshini-kumari/perceptNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a target="_blank" href="/assets/pdf/whc2019_slide.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>In order to design haptic icons or build a haptic vocabulary, we require a set of easily distinguishable haptic signals to avoid perceptual ambiguity, which in turn requires a way to accurately estimate the perceptual (dis)similarity of such signals. In this work, we present a novel method to learn such a perceptual metric based on data from human studies. Our method is based on a deep neural network that projects signals to an embedding space where the natural Euclidean distance accurately models the degree of dissimilarity between two signals. The network is trained only on non-numerical comparisons of triplets of signals, using a novel triplet loss that considers both types of triplets that are easy to order (inequality constraints), as well as those that are unorderable/ambiguous (equality constraints). Unlike prior MDS-based non-parametric approaches, our method can be trained on a partial set of comparisons and can embed new haptic signals without retraining the model from scratch. Extensive experimental evaluations show that our method is significantly more effective at modeling perceptual dissimilarity than alternatives.</p> </div> <p>We propose a deep metric learning approach that demonstrates the utility of ambiguous triplets for effectively modeling human perception. </p> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%70%72%69%79%61%64%61%72%73%68%69%6E%69.%6B%75%6D%61%72%69@%73%6F%6E%79.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=mD0burkAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.linkedin.com/in/priyadarshini-kumari-501b65162" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a target="_blank" href="assets/pdf/priyadarshini_cv.pdf" title="Work"><i class="ai ai-cv ai-1x"></i></a> </div> <div class="contact-note">Feel free to drop a line via email </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Priyadarshini . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: September 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-D0DWPWGDV2"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-D0DWPWGDV2");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>