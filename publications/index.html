<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Priyadarshini Kumari </title> <meta name="author" content="Priyadarshini "> <meta name="description" content="My Webpage "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://www.priyadarshini-k.com/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Priyadarshini Kumari </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ds1-480.webp 480w,/assets/img/publication_preview/ds1-800.webp 800w,/assets/img/publication_preview/ds1-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ds1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ds1.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">AI Review</a></abbr> </div> <div id="priyadarshini2024ds1" class="col-sm-8"> <div class="title">Link prediction for hypothesis generation: an active curriculum learning infused temporal graph-based approach</div> <div class="author"> Uchenna Akujuobi* , <em>Priyadarshini Kumari</em>, Jihun Choi , Samy Badreddine , Kana Maruyama , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Sucheendra K. Palaniappan, Tarek R. Besold' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.text(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Artificial Intelligence Review</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/ds1.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Over the last few years Literature-based Discovery (LBD) has regained popularity as a means to enhance the scientific research process. The resurgent interest has spurred the development of supervised and semi-supervised machine learning models aimed at making previously implicit connections between scientific concepts/entities explicit based on often extensive repositories of published literature. Understanding the temporally evolving interactions between these entities can provide valuable information for predicting the future development of entity relationships. However, existing methods often underutilize the latent information embedded in the temporal aspects of interaction data. In this context, motivated by applications in the food domain—where we aim to connect nutritional information with health-related benefits—we address the hypothesis-generation problem using a temporal graph-based approach. Given that hypothesis generation involves predicting future (i.e., still to be discovered) entity connections, the ability to capture the dynamic evolution of connections over time is pivotal for a robust model. To address this, we introduce THiGER, a novel batch contrastive temporal node-pair embedding method. THiGER excels in providing a more expressive node-pair encoding by effectively harnessing node-pair relationships. Furthermore, we present THiGER-A, an incremental training approach that incorporates an active curriculum learning strategy to mitigate label bias arising from unobserved connections. By progressively training on increasingly challenging and high-utility samples, our approach significantly enhances the performance of the embedding model. Empirical validation of our proposed method demonstrates its effectiveness on established temporal-graph benchmark datasets, as well as on real-world datasets within the food domain.</p> </div> <p>We present THiGER-A, a solution aimed at addressing the hypothesis-generation problem. This method utilizes a temporal graph-based node-pair embedding and incorporates an active-curriculum training approach to precisely capture the dynamic evolution of discoveries over time.</p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/bmvc-480.webp 480w,/assets/img/publication_preview/bmvc-800.webp 800w,/assets/img/publication_preview/bmvc-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/bmvc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="bmvc.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">BMVC</a></abbr> </div> <div id="priyadarshini2024bmvc" class="col-sm-8"> <div class="title">CosFairNet:A Parameter-Space based Approach for Bias Free Learning</div> <div class="author"> Rajeev Ranjan Dwivedi , <em>Priyadarshini Kumari</em>, and Vinod Kumar Kurmi </div> <div class="periodical"> <em>In British Machine Vision Conference</em> , 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/Fairnesss_BMVC_2024.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Deep neural networks trained on biased data often inadvertently learn unintended inference rules, particularly when labels are strongly correlated with biased features. Existing bias mitigation methods typically involve either a) predefining bias types and enforcing them as prior knowledge or b) reweighting training samples to emphasize bias-conflicting samples over bias-aligned samples. However, both strategies address bias indirectly in the feature or sample space, with no control over learned weights, making it difficult to control the bias propagation across different layers. Based on this observation, we introduce a novel approach to address bias directly in the model’s parameter space, preventing its propagation across layers. Our method involves training two models: a bias model for biased features and a debias model for unbiased details, guided by the bias model. We enforce dissimilarity in the debias model’s later layers and similarity in its initial layers with the bias model, ensuring it learns unbiased low-level features without adopting biased high-level abstractions. By incorporating this explicit constraint during training, our approach shows enhanced classification accuracy and debiasing effectiveness across various synthetic and real-world datasets of different sizes. Moreover, the proposed method demonstrates robustness across different bias types and percentages of biased samples in the training data.</p> </div> <p></p> </div> </div> </li> </ol> <h2 class="year">2023</h2> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/fruni_ftree-480.webp 480w,/assets/img/publication_preview/fruni_ftree-800.webp 800w,/assets/img/publication_preview/fruni_ftree-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/fruni_ftree.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="fruni_ftree.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">NeurIPS XAIA</a></abbr> </div> <div id="priyadarshini2023neurips" class="col-sm-8"> <div class="title">FRUNI and FTREE synthetic knowledge graphs for evaluating explainability</div> <div class="author"> Pablo Sanchez Martin , Tarek Besold , and <em>Priyadarshini Kumari</em> </div> <div class="periodical"> <em>In NeurIPS 2023 Workshop XAIA</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/fruni_and_ftree.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/SonyResearch/synthetic_knowledge_graphs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Research on knowledge graph completion (KGC)—i.e., link prediction within incomplete KGs—is witnessing significant growth in popularity. Recently, KGC using KG embedding (KGE) models, primarily based on complex architectures (e.g., transformers), have achieved remarkable performance. Still, extracting the \emphminimal and relevant information employed by KGE models to make predictions, while constituting a major part of \emphexplaining the predictions, remains a challenge. While there exists a growing literature on explainers for trained KGE models, systematically exposing and quantifying their failure cases poses even greater challenges. In this work, we introduce two synthetic datasets, FRUNI and FTREE, designed to demonstrate the (in)ability of explainer methods to spot link predictions that rely on indirectly connected links. Notably, we empower practitioners to control various aspects of the datasets, such as noise levels and dataset size, enabling them to assess the performance of explainability methods across diverse scenarios. Through our experiments, we assess the performance of four recent explainers in providing accurate explanations for predictions on the proposed datasets. We believe that these datasets are valuable resources for further validating explainability methods within the knowledge graph community.</p> </div> <p>We introduce two synthetic datasets, FRUNI and FTREE, to assess explainer methods’ ability to identify predictions relying on indirectly connected links.</p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/label_balancer_method-480.webp 480w,/assets/img/publication_preview/label_balancer_method-800.webp 800w,/assets/img/publication_preview/label_balancer_method-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/label_balancer_method.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="label_balancer_method.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">Multimodal SIGKDD</a></abbr> </div> <div id="priyadarshini2023aromasense" class="col-sm-8"> <div class="title">Optimizing Learning Across Multimodal Transfer Features for Modeling Olfactory Perception</div> <div class="author"> Daniel Shin , Gao Pei , <em>Priyadarshini Kumari</em>, and Tarek Besold </div> <div class="periodical"> <em>In International Workshop on Multimodal Learning at SIGKDD</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/Multimodal_KDD_2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/priyadarshini-sony/multimodal-olfaction" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a target="_blank" href="/assets/pdf/multimodal_SIGKDD_2023_slides.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>For humans and other animals, the sense of smell provides crucial information in many situations of everyday life. Still, the study of olfactory perception has received only limited attention outside of the biological sciences. From an AI perspective, the complexity of the interactions between olfactory receptors and volatile molecules and the scarcity of comprehensive olfactory datasets, present unique challenges in this sensory domain. Previous works have explored the relationship between molecular structure and odor descriptors using fully supervised training approaches. However, these methods are data-intensive and poorly generalize due to labeled data scarcity, particularly for rare-class samples. Our study partially tackles the challenges of data scarcity and label skewness through multimodal transfer learning. We investigate the potential of large molecular foundation models trained on extensive unlabeled molecular data to effectively model olfactory perception. Additionally, we explore the integration of different molecular representations, including molecular graphs and text-based SMILES encodings, to achieve data efficiency and generalization of the learned model, particularly on sparsely represented classes. By leveraging complementary representations, we aim to learn robust perceptual features of odorants. However, we observe that traditional methods of combining modalities do not yield substantial gains in high-dimensional skewed label spaces. To address this challenge, we introduce a novel label-balancer technique specifically designed for high-dimensional multi-label and multi-modal training. The label-balancer technique distributes learning objectives across modalities to optimize collaboratively for distinct subsets of labels. Our results suggest that multi-modal transfer features learned using the label-balancer technique are more effective and robust, surpassing the capabilities of traditional uni- or multi-modal approaches, particularly on rare-class samples.</p> </div> <p>We introduce a novel multilabel and multimodal transfer learning technique for modeling olfactory perception. Our approach aims to tackle the challenges of data scarcity and label skewness in the olfactory domain.</p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/perceptual_metric-480.webp 480w,/assets/img/publication_preview/perceptual_metric-800.webp 800w,/assets/img/publication_preview/perceptual_metric-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/perceptual_metric.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="perceptual_metric.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">PLOS ONE</a></abbr> </div> <div id="priyadarshini2023new2" class="col-sm-8"> <div class="title">Perceptual metrics for odorants: learning from non-expert similarity feedback using machine learning</div> <div class="author"> <em>Priyadarshini Kumari</em>, Tarek Besold , and Michael Spranger </div> <div class="periodical"> <em>In PLOS ONE</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/plosone_perceptual.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Defining perceptual similarity metrics for odorant comparisons is crucial to understanding the mechanism of olfactory perception. Current methods in olfaction rely on molecular physicochemical features or discrete verbal descriptors (floral, burnt, etc.) to approximate perceptual (dis)similarity between odorants. However, structural or verbal descriptors alone are limited in modeling complex nuances of odor perception. While structural features inadequately characterize odor perception, language-based discrete descriptors lack the granularity needed to model a continuous perception space. We introduce data-driven approaches to perceptual metrics learning (PMeL) based on two key insights: a) by combining physicochemical features with the user’s perceptual feedback, we can leverage both structural and perceptual attributes of odors to define dissimilarity, and b) instead of discrete labels, user’s perceptual feedback can be gathered as relative similarity comparisons, such as “Does molecule-A smell more like molecule-B, or molecule-C?" These triplet comparisons are easier even for non-experts users and offer a more effective representation of the continuous perception space. Experimental results on several defined tasks show the effectiveness of our approach in evaluating perceptual dissimilarity between odorants. Finally, we investigate how closely our model, trained on non-expert feedback, aligns with the expert’s similarity judgments. Our effort aims to reduce reliance on expert annotations.</p> </div> <p> We propose Perceptual Metrics Learning (PMeL) for olfactory perception, which combines physicochemical features with user feedback via triplet comparisons to assess perceptual dissimilarity effectively and reduce reliance, to some extent, on expert annotations.</p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/plosone-480.webp 480w,/assets/img/publication_preview/plosone-800.webp 800w,/assets/img/publication_preview/plosone-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/plosone.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="plosone.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">PLOS ONE</a></abbr> </div> <div id="priyadarshini2023new3" class="col-sm-8"> <div class="title">Comparing molecular representations, e-nose signals, and other featurization, for learning to smell aroma molecules</div> <div class="author"> Tanoy Debnath , Samy Badreddine , <em>Priyadarshini Kumari</em>, and Michael Spranger </div> <div class="periodical"> <em>In PLOS ONE</em> , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/plosone2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>Recent research has attempted to predict our perception of odorants using Machine Learning models. The featurization of the olfactory stimuli usually represents the odorants using molecular structure parameters, molecular fingerprints, mass spectra, or e-nose signals. However, the impact of the choice of featurization on predictive performance remains poorly reported in direct comparative studies. This paper experiments with different sensory fea- tures for several olfactory perception tasks. We investigate the multilabel classification of aroma molecules in odor descriptors. We investigate single-label classification not only in fine-grained odor descriptors ("orange," "waxy," etc.) but also in odor descriptor groups. We created a database of odor vectors for 114 aroma molecules to conduct our experiments using a QCM (Quartz Crystal Microbalance) type smell sensor module (Aroma Coder®V2 Set). We compare these smell features with different baseline features to evaluate the cluster composition, considering the frequencies of the top odor descriptors carried by the aroma molecules. Experimental results suggest a statistically significant better performance of the QCM-type smell sensor module compared with other baseline features with the F1 evaluation metric.</p> </div> </div> </div> </li> </ol> <h2 class="year">2022</h2> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/hapsymposium2022-480.webp 480w,/assets/img/publication_preview/hapsymposium2022-800.webp 800w,/assets/img/publication_preview/hapsymposium2022-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/hapsymposium2022.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hapsymposium2022.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">IEEE Haptics Symposium</a></abbr> </div> <div id="priyadarshini2022enhancing" class="col-sm-8"> <div class="title">Enhancing Haptic Distinguishability of Surface Materials With Boosting Technique</div> <div class="author"> <em>Priyadarshini Kumari</em>, and Subhasis Chaudhuri </div> <div class="periodical"> <em>In IEEE Haptics Symposium</em> , 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/symposium2022.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a target="_blank" href="/assets/pdf/hapsymposium2022_slide.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Discriminative features are crucial for several learning applications, such as object detection and classification. Neural networks are extensively used for extracting discriminative features of images and speech signals. However, the lack of large datasets in the haptics domain often limits the applicability of such techniques. This paper presents a general framework for the analysis of the discriminative properties of haptic signals. We demonstrate the effectiveness of spectral features and a boosted embedding technique in enhancing the distinguishability of haptic signals. Experiments indicate our framework needs less training data, generalizes well for different predictors, and outperforms the related state-of-the-art.</p> </div> <p>We develop a discriminative feature learning framework to improve separability for high-dimensional and highly correlated haptic signals.</p> </div> </div> </li></ol> <h2 class="year">2021</h2> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ecml2021-480.webp 480w,/assets/img/publication_preview/ecml2021-800.webp 800w,/assets/img/publication_preview/ecml2021-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ecml2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ecml2021.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">ECML-PKDD</a></abbr> </div> <div id="priyadarshini2021unified" class="col-sm-8"> <div class="title">A Unified Batch Selection Policy for Active Metric Learning</div> <div class="author"> <em>Priyadarshini Kumari</em>, Siddhartha Chaudhuri , Vivek Borkar , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In ECML-PKDD</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/ecml.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a target="_blank" href="/assets/pdf/ecml2021_slide.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Active metric learning is the problem of incrementally selecting high-utility batches of training data (typically, ordered triplets) to annotate, in order to progressively improve a learned model of a metric over some input domain as rapidly as possible. Standard approaches, which independently assess the informativeness of each triplet in a batch, are susceptible to highly correlated batches with many redundant triplets and hence low overall utility. While a recent work proposes batch-decorrelation strategies for metric learning, they rely on ad hoc heuristics to estimate the correlation between two triplets at a time. We present a novel batch active metric learning method that leverages the Maximum Entropy Principle to learn the least biased estimate of triplet distribution for a given set of prior constraints. To avoid redundancy between triplets, our method collectively selects batches with maximum joint entropy, which simultaneously captures both informativeness and diversity. We take advantage of the submodularity of the joint entropy function to construct a tractable solution using an efficient greedy algorithm based on Gram-Schmidt orthogonalization that is provably (1−\frac1𝑒)-optimal. Our approach is the first batch active metric learning method to define a unified score that balances informativeness and diversity for an entire batch of triplets. Experiments with several real-world datasets demonstrate that our algorithm is robust, generalizes well to different applications and input modalities, and consistently outperforms the state-of-the-art.</p> </div> <p>We propose a batch-mode active learning method that balances informativeness and diversity of batches of triplets combinedly. </p> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ijcai2020-480.webp 480w,/assets/img/publication_preview/ijcai2020-800.webp 800w,/assets/img/publication_preview/ijcai2020-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/ijcai2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ijcai2020.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">IJCAI</a></abbr> </div> <div id="kumari2021batch" class="col-sm-8"> <div class="title">Batch decorrelation for active metric learning</div> <div class="author"> <em>Priyadarshini Kumari</em>, Ritesh Goru , Siddhartha Chaudhuri , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In IJCAI</em> , 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/ijcai.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/priyadarshini-kumari/BatchAML_Decorrelation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a target="_blank" href="/assets/pdf/ijcai2020_slide.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>We present an active learning strategy for training parametric models of distance metrics, given triplet-based similarity assessments: object xi is more similar to object xj than to xk. In contrast to prior work on class-based learning, where the fundamental goal is classification and any implicit or explicit metric is binary, we focus on perceptual metrics that express the degree of (dis) similarity between objects. We find that standard active learning approaches degrade when annotations are requested for batches of triplets at a time: our studies suggest that correlation among triplets is responsible. In this work, we propose a novel method to decorrelate batches of triplets, that jointly balances informativeness and diversity while decoupling the choice of heuristic for each criterion. Experiments indicate our method is general, adaptable, and outperforms the state-of-the-art.</p> </div> <p>We propose triplet-based decorrelation measures to improve the performance of batch-mode active metric learning strategies. </p> </div> </div> </li> </ol> <h2 class="year">2019</h2> <h2 class="bibliography">2019</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/whc2019-480.webp 480w,/assets/img/publication_preview/whc2019-800.webp 800w,/assets/img/publication_preview/whc2019-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/whc2019.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="whc2019.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">IEEE WHC</a></abbr> </div> <div id="priyadarshini2019perceptnet" class="col-sm-8"> <div class="title">PerceptNet: Learning perceptual similarity of haptic textures in presence of unorderable triplets</div> <div class="author"> <em>Priyadarshini Kumari</em>, Siddhartha Chaudhuri , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In IEEE World Haptics Conference (WHC)</em> , 2019 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/whc2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/priyadarshini-kumari/perceptNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a target="_blank" href="/assets/pdf/whc2019_slide.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>In order to design haptic icons or build a haptic vocabulary, we require a set of easily distinguishable haptic signals to avoid perceptual ambiguity, which in turn requires a way to accurately estimate the perceptual (dis)similarity of such signals. In this work, we present a novel method to learn such a perceptual metric based on data from human studies. Our method is based on a deep neural network that projects signals to an embedding space where the natural Euclidean distance accurately models the degree of dissimilarity between two signals. The network is trained only on non-numerical comparisons of triplets of signals, using a novel triplet loss that considers both types of triplets that are easy to order (inequality constraints), as well as those that are unorderable/ambiguous (equality constraints). Unlike prior MDS-based non-parametric approaches, our method can be trained on a partial set of comparisons and can embed new haptic signals without retraining the model from scratch. Extensive experimental evaluations show that our method is significantly more effective at modeling perceptual dissimilarity than alternatives.</p> </div> <p>We propose a deep metric learning approach that demonstrates the utility of ambiguous triplets for effectively modeling human perception. </p> </div> </div> </li></ol> <h2 class="year">2017</h2> <h2 class="bibliography">2017</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/book-480.webp 480w,/assets/img/publication_preview/book-800.webp 800w,/assets/img/publication_preview/book-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/book.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="book.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">Book</a></abbr> </div> <div id="chaudhuri2017cultural" class="col-sm-8"> <div class="title">Cultural heritage objects: Bringing them alive through virtual touch</div> <div class="author"> Subhasis Chaudhuri , and <em>Priyadarshini Kumari</em> </div> <div class="periodical"> <em>In Digital Hampi: Preserving Indian Cultural Heritage</em> , 2017 </div> <div class="periodical"> </div> <div class="links"> <a target="_blank" href="/assets/pdf/book.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <p>We develop a multi-modal rendering framework to provide haptic, visual, and auditory perception of objects at different scales and rotations.</p> </div> </div> </li></ol> <h2 class="year">2016</h2> <h2 class="bibliography">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eurohaptics2016-480.webp 480w,/assets/img/publication_preview/eurohaptics2016-800.webp 800w,/assets/img/publication_preview/eurohaptics2016-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/eurohaptics2016.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eurohaptics2016.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">EuroHaptics</a></abbr> </div> <div id="kumari2016haptic" class="col-sm-8"> <div class="title">Haptic Rendering of Thin, Deformable Objects with Spatially Varying Stiffness</div> <div class="author"> <em>Priyadarshini Kumari</em>, and Subhasis Chaudhuri </div> <div class="periodical"> <em>In EuroHaptics</em> , 2016 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/eurohaptics2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In real world, we often come across with soft objects having spatially varying stiffness such as human palm or a wart on the skin. In this paper, we propose a novel approach to render thin, deformable objects having spatially varying stiffness (inhomogeneous material). We use the classical Kirchhoff thin plate theory to compute the deformation. In general, physics based rendering of an arbitrary 3D surface is complex and time consuming. Therefore, we approximate the 3D surface locally by a 2D plane using an area preserving mapping technique - Gall-Peters mapping. Once the deformation is computed by solving a fourth order partial differential equation, we project the points back onto the original object for proper haptic rendering. The method was validated through user experiments and was found to be realistic.</p> </div> <p>We introduce a haptic rendering algorithm to model deformation in objects with varying stiffness. </p> </div> </div> </li></ol> <h2 class="year">2014</h2> <h2 class="bibliography">2014</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/accvw-480.webp 480w,/assets/img/publication_preview/accvw-800.webp 800w,/assets/img/publication_preview/accvw-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/accvw.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="accvw.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">ACCVw</a></abbr> </div> <div id="aniyath2014combined" class="col-sm-8"> <div class="title">Combined hapto-visual and auditory rendering of cultural heritage objects</div> <div class="author"> Praseedha Krishnan Aniyath , Sreeni Kamalalayam Gopalan , <em>Priyadarshini Kumari</em>, and Subhasis Chaudhuri </div> <div class="periodical"> <em>In ACCVw</em> , 2014 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/accv.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this work, we develop a multi-modal rendering framework comprising of hapto-visual and auditory data. The prime focus is to haptically render point cloud data representing virtual 3-D models of cultural significance and also to handle their affine transformations. Cultural heritage objects could potentially be very large and one may be required to render the object at various scales of details. Further, surface effects such as texture and friction are incorporated in order to provide a realistic haptic perception to the users. Moreover, the proposed framework includes an appropriate sound synthesis to bring out the acoustic properties of the object. It also includes a graphical user interface with varied options such as choosing the desired orientation of 3-D objects and selecting the desired level of spatial resolution adaptively at runtime. A fast, point proxy-based haptic rendering technique is proposed with proxy update loop running 100 times faster than the required haptic update frequency of 1 kHz. The surface properties are integrated in the system by applying a bilateral filter on the depth data of the virtual 3-D models. Position dependent sound synthesis is incorporated with the incorporation of appropriate audio clips.</p> </div> </div> </div> </li></ol> <h2 class="year">2013</h2> <h2 class="bibliography">2013</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/whc2013-480.webp 480w,/assets/img/publication_preview/whc2013-800.webp 800w,/assets/img/publication_preview/whc2013-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/whc2013.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="whc2013.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">IEEE WHC</a></abbr> </div> <div id="kumari2013scalable" class="col-sm-8"> <div class="title">Scalable rendering of variable density point cloud data</div> <div class="author"> <em>Priyadarshini Kumari</em>, KG Sreeni , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In IEEE World Haptics Conference (WHC)</em> , 2013 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/whc2013.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this paper, we present a novel proxy based method of adaptive haptic rendering of a variable density 3D point cloud data at different levels of detail without pre-computing the mesh structure. We also incorporate features like rotation, translation and friction to provide a better realistic experience to the user. A proxy based rendering technique is used to avoid the pop-through problem while rendering thin parts of the object. Instead of a point proxy, a spherical proxy of variable radius is used which avoids the sinking of proxy during the haptic interaction of sparse data. The radius of the proxy is adaptively varied depending upon the local density of the point data using kernel bandwidth estimation. During the interaction, the proxy moves in small steps tangentially over the point cloud such that the new position always minimizes the distance between the proxy and the haptic interaction point (HIP). The raw point cloud data re-sampled in a regular 3D lattice of voxels are loaded to the haptic space after proper smoothing to avoid aliasing effects. The rendering technique is experimented with several subjects and it is observed that this functionality supplements the user’s experience by allowing the user to interact with an object at multiple resolutions.</p> </div> </div> </div> </li></ol> <h2 class="year">2012</h2> <h2 class="bibliography">2012</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 preview"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/eurohaptics2012-480.webp 480w,/assets/img/publication_preview/eurohaptics2012-800.webp 800w,/assets/img/publication_preview/eurohaptics2012-1400.webp 1400w," sizes="200px" type="image/webp"></source> <img src="/assets/img/publication_preview/eurohaptics2012.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="eurohaptics2012.gif" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <abbr class="badge"> <a href="">EuroHaptics</a></abbr> </div> <div id="sreeni2012haptic" class="col-sm-8"> <div class="title">Haptic rendering of cultural heritage objects at different scales</div> <div class="author"> KG Sreeni , <em>Priyadarshini Kumari</em>, AK Praseedha , and Subhasis Chaudhuri </div> <div class="periodical"> <em>In EuroHaptics</em> , 2012 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a target="_blank" href="/assets/pdf/eurohaptics2012.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>In this work, we address the issue of virtual representation of objects of cultural heritage for haptic interaction. Our main focus is to provide a haptic access of artistic objects of any physical scale to the differently abled people. This is a low-cost system and, in conjunction with a stereoscopic visual display, gives a better immersive experience even to the sighted persons. To achieve this, we propose a simple multilevel, proxy-based hapto-visual rendering technique for point cloud data which includes the much desired scalability feature which enables the users to change the scale of the objects adaptively during the haptic interaction. For the proposed haptic rendering technique the proxy updation loop runs at a rate 100 times faster than the required haptic updation frequency of 1KHz. We observe that this functionality augments very well to the realism of the experience.</p> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Priyadarshini . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: September 16, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-D0DWPWGDV2"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-D0DWPWGDV2");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>